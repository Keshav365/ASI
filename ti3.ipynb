{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in ./lib/python3.13/site-packages (0.32.2)\n",
      "Requirement already satisfied: transformers in ./lib/python3.13/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in ./lib/python3.13/site-packages (2.6.0)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: importlib-metadata in ./lib/python3.13/site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: filelock in ./lib/python3.13/site-packages (from diffusers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in ./lib/python3.13/site-packages (from diffusers) (0.29.1)\n",
      "Requirement already satisfied: numpy in ./lib/python3.13/site-packages (from diffusers) (2.2.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./lib/python3.13/site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./lib/python3.13/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./lib/python3.13/site-packages (from diffusers) (0.5.2)\n",
      "Requirement already satisfied: Pillow in ./lib/python3.13/site-packages (from diffusers) (11.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./lib/python3.13/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.13/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: psutil in ./lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./lib/python3.13/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.13/site-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.13/site-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.13/site-packages (from requests->diffusers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.13/site-packages (from requests->diffusers) (2025.1.31)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch.optim import AdamW\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "placeholder_token = \"icon\"\n",
    "learnable_property = \"object\"\n",
    "num_train_epochs = 1000\n",
    "checkpoint_steps = 100  # Save checkpoint every 100 steps\n",
    "learning_rate = 5e-4\n",
    "output_dir = \"textual_inversion_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "with open(\"/Users/xs518-shigoy/Documents/AI Agent/.TIFewImage/images/metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'An orange light bulb icon with blue rays emanating from it.',\n",
       "  'image': '08-05-2024 Icons for AI Library-01.png'},\n",
       " {'text': 'A web browser window with three interlocking gears in blue orange and white.',\n",
       "  'image': '08-05-2024 Icons for AI Library-02.png'},\n",
       " {'text': 'Three circular icons with a blue gear green chat bubble and orange wrench',\n",
       "  'image': '08-05-2024 Icons for AI Library-03.png'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./lib/python3.13/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./lib/python3.13/site-packages (from accelerate) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lib/python3.13/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in ./lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./lib/python3.13/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./lib/python3.13/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./lib/python3.13/site-packages (from accelerate) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./lib/python3.13/site-packages (from accelerate) (0.5.2)\n",
      "Requirement already satisfied: filelock in ./lib/python3.13/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./lib/python3.13/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.2.0)\n",
      "Requirement already satisfied: requests in ./lib/python3.13/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./lib/python3.13/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./lib/python3.13/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./lib/python3.13/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./lib/python3.13/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lib/python3.13/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./lib/python3.13/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.13/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Error while downloading from https://cdn-lfs.hf.co/repos/66/6f/666f465fa70158515404e8de2c6bc6fe2f90c46f9296293aa14daededeb32c52/19da7aaa4b880e59d56843f1fcb4dd9b599c28a1d9d9af7c1143057c8ffae9f1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27diffusion_pytorch_model.safetensors%3B+filename%3D%22diffusion_pytorch_model.safetensors%22%3B&Expires=1740120096&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDEyMDA5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82Ni82Zi82NjZmNDY1ZmE3MDE1ODUxNTQwNGU4ZGUyYzZiYzZmZTJmOTBjNDZmOTI5NjI5M2FhMTRkYWVkZWRlYjMyYzUyLzE5ZGE3YWFhNGI4ODBlNTlkNTY4NDNmMWZjYjRkZDliNTk5YzI4YTFkOWQ5YWY3YzExNDMwNTdjOGZmYWU5ZjE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=In44EDYR3amcRuaav56Q-mjP-HEywrzzfh%7Em7GEpIV%7EYtI4tci5nXf1XmeoahnDda3tOHe1YhNEjIVBXDe4v7z0Oa6T9hg0EdkfKAKZL4uTl5uQOnSYGJdDp%7El2hSdTnO7miTQKsG755QlpcNEhNbLvml-giPkSbx1OOFE61fJT01yMouKrvvD2Oh04DQZxXCRk3TOH1khcp1b88sWhGzq2uJgjLDCT3zarUW1XYCgumV8c%7EtGYDO7YjtWp0-npjjaEpW38GxoPc5Lzve0SJSUtaLnDh%7EsXnsXQLJkJmn2aYz8p0NQDFXA2ZZJ%7Ezv4F4nk7SAewWq%7EvOC7CDuGXG8Q__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add placeholder token\n",
    "num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(f\"Token {placeholder_token} already exists\")\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token ID\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "# Freeze all parameters except the token embeddings\n",
    "text_encoder.text_model.encoder.requires_grad_(False)\n",
    "text_encoder.text_model.final_layer_norm.requires_grad_(False)\n",
    "text_encoder.text_model.embeddings.position_embedding.requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new token with random embeddings\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = torch.randn_like(token_embeds[placeholder_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "dataset = []\n",
    "for item in metadata:\n",
    "    image = Image.open(item[\"file_name\"])\n",
    "    image = image.convert(\"RGB\").resize((512, 512))\n",
    "    caption = item[\"caption\"]  # Fixed bracket\n",
    "    dataset.append((image, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "optimizer = AdamW(text_encoder.get_input_embeddings().parameters(), lr=learning_rate)\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_train_epochs):\n",
    "    for image, caption in dataset:\n",
    "        # Convert image to latents\n",
    "        with torch.no_grad():\n",
    "            latents = torch.randn((1, 4, 64, 64))  # Simplified for example\n",
    "            \n",
    "        # Tokenize text\n",
    "        input_ids = tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        # Forward pass\n",
    "        text_encoder.train()\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (1,))\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Predict noise\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "\n",
    "        # Save checkpoint\n",
    "        if global_step % checkpoint_steps == 0:\n",
    "            checkpoint_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"global_step\": global_step,\n",
    "                \"state_dict\": text_encoder.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_train_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final embeddings\n",
    "text_encoder.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"textual_inversion_model/checkpoint-100\")\n",
    "text_encoder.load_state_dict(checkpoint[\"state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".TIFewImage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
